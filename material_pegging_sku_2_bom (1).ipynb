{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Pegging Map - Per-SKU Pegging with Complete Hierarchy\n",
    "## Each Market SKU sheet shows: Packing -> Assembly -> Filling hierarchy\n",
    "\n",
    "**Output:** Material_pegging_SKU_2_BOM.xlsx\n",
    "\n",
    "**Sheet Structure:** Pegging_{8-series SKU}\n",
    "\n",
    "**Hierarchy per SKU:**\n",
    "- Level 1: Market SKU (800004403) + Materials\n",
    "- Separator: 0\n",
    "- Level 2: Assembly (700003964) + Materials\n",
    "- Separator: 0\n",
    "- Level 3: Filling (700001012) + Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(\"MATERIAL PEGGING MAP - PER-SKU PEGGING WITH COMPLETE HIERARCHY\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "dp_file = \"/home/supriyo/Downloads/Biocon_nw/20251006-DP Material Shortage - Working file.xlsx\"\n",
    "snp_file = \"/home/supriyo/Downloads/Biocon_nw/ParkourSC_SNP.xlsx\"\n",
    "pegging_file = \"/home/supriyo/Downloads/Biocon_nw/Material_pegging_SKU_2_BOM.xlsx\"\n",
    "\n",
    "print(f\"Output: {output_file}\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/supriyo/repogen/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return None\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    return text if text else None\n",
    "\n",
    "def normalize_product_no(value):\n",
    "    if pd.isna(value) or value is None:\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "    return cleaned if cleaned else None\n",
    "\n",
    "def extract_model_components(model_text):\n",
    "    if pd.isna(model_text):\n",
    "        return None\n",
    "    model_text = str(model_text).strip()\n",
    "    components = re.split(r'_+', model_text)\n",
    "    components = [c.strip() for c in components if c.strip()]\n",
    "    return '_'.join(components)\n",
    "\n",
    "def is_valid_qty(qty):\n",
    "    if pd.isna(qty):\n",
    "        return False\n",
    "    qty_str = str(qty).strip()\n",
    "    if not qty_str or qty_str == '0' or qty_str == 'nan':\n",
    "        return False\n",
    "    try:\n",
    "        return float(qty_str) > 0\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "print(\"Functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "df_headers = pd.read_excel(dp_file, sheet_name=\"DP Shortage\", header=None, skiprows=18, nrows=4, usecols=range(14, 135))\n",
    "product_headers = {}\n",
    "for col_idx in range(df_headers.shape[1]):\n",
    "    product_id = normalize_product_no(df_headers.iloc[1, col_idx])\n",
    "    if product_id and product_id != '0':\n",
    "        product_headers[product_id] = {\n",
    "            'Product_ID': product_id,\n",
    "            'Product_Description': normalize_text(df_headers.iloc[3, col_idx]),\n",
    "            'Batch_Size': df_headers.iloc[0, col_idx],\n",
    "            'Column_Index': col_idx + 14\n",
    "        }\n",
    "\n",
    "print(f\"Product headers: {len(product_headers)}\")\n",
    "\n",
    "df_materials = pd.read_excel(dp_file, sheet_name=\"DP Shortage\", header=None, skiprows=22, nrows=520, usecols=[0, 1, 2, 3, 4, 5, 10, 13])\n",
    "df_materials.columns = ['Material', 'Material_Description', 'Model', 'Product_Family', 'Section', 'Common_Unique', 'Total_Lead_Time', 'BUoM']\n",
    "df_materials['Material_Normalized'] = df_materials['Material'].apply(normalize_product_no)\n",
    "df_materials_filtered = df_materials[(df_materials['Material_Normalized'].notna()) & (df_materials['Material_Normalized'] != '0')].copy()\n",
    "print(f\"Materials: {len(df_materials_filtered)}\")\n",
    "\n",
    "df_qty = pd.read_excel(dp_file, sheet_name=\"DP Shortage\", header=None, skiprows=22, nrows=520, usecols=range(14, 135))\n",
    "qty_col_map = {}\n",
    "for product_id, info in product_headers.items():\n",
    "    qty_col_map[product_id] = info['Column_Index'] - 14\n",
    "\n",
    "print(\"Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting materials per product...\")\n",
    "product_materials = {}\n",
    "\n",
    "for product_id, col_idx_in_qty in qty_col_map.items():\n",
    "    qty_values = df_qty.iloc[:, col_idx_in_qty]\n",
    "    valid_qty_mask = qty_values.apply(is_valid_qty)\n",
    "    valid_row_indices = df_materials_filtered.index[valid_qty_mask[df_materials_filtered.index]].tolist()\n",
    "    \n",
    "    if len(valid_row_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    materials_for_product = df_materials_filtered.loc[valid_row_indices].copy()\n",
    "    materials_for_product['QTY'] = qty_values[valid_row_indices].values\n",
    "    materials_for_product['Product_ID'] = product_id\n",
    "    product_materials[product_id] = materials_for_product\n",
    "\n",
    "print(f\"Extracted for {len(product_materials)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Resource and SKU data...\")\n",
    "\n",
    "resource_data = {}\n",
    "try:\n",
    "    df_resources = pd.read_excel(snp_file, sheet_name=\"DP Line Utilization\", header=None, skiprows=2, nrows=240, usecols=[1, 2, 4])\n",
    "    df_resources.columns = ['Resource_ID', 'Resource_Description', 'Product_ID']\n",
    "    for _, row in df_resources.iterrows():\n",
    "        prod_id = normalize_product_no(row['Product_ID'])\n",
    "        if prod_id:\n",
    "            resource_data[prod_id] = {\n",
    "                'Resource_ID': normalize_text(row['Resource_ID']),\n",
    "                'Resource_Description': normalize_text(row['Resource_Description'])\n",
    "            }\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "sku_data = {}\n",
    "try:\n",
    "    df_adv = pd.read_excel(snp_file, sheet_name=\"Adv Mkt-Mar'25\", header=None, skiprows=2, nrows=363, usecols=[1, 3, 5, 8])\n",
    "    df_adv.columns = ['Product_ID', 'SKU', 'Country', 'Pack_Size']\n",
    "    for _, row in df_adv.iterrows():\n",
    "        prod_id = normalize_product_no(row['Product_ID'])\n",
    "        if prod_id and prod_id not in sku_data:\n",
    "            sku_data[prod_id] = {'SKU': normalize_text(row['SKU']), 'Country': normalize_text(row['Country']), 'Pack_Size': row['Pack_Size']}\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df_em = pd.read_excel(snp_file, sheet_name=\"EM-Mar'25\", header=None, skiprows=2, nrows=44, usecols=[1, 6, 12, 14])\n",
    "    df_em.columns = ['Product_ID', 'SKU', 'Country', 'Pack_Size']\n",
    "    for _, row in df_em.iterrows():\n",
    "        prod_id = normalize_product_no(row['Product_ID'])\n",
    "        if prod_id and prod_id not in sku_data:\n",
    "            sku_data[prod_id] = {'SKU': normalize_text(row['SKU']), 'Country': normalize_text(row['Country']), 'Pack_Size': row['Pack_Size']}\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"Resource data: {len(resource_data)}\")\n",
    "print(f\"SKU data: {len(sku_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining hierarchy...\")\n",
    "\n",
    "product_mapping = {}\n",
    "\n",
    "mCB_skus = ['800004403', '800004402', '800008019', '800008020', '800008034', '800007997', '800007345', '800007516',\n",
    "            '800002513', '800007608', '800007630', '800002984', '800004986', '800007310', '800007311', '800006648',\n",
    "            '800007634', '800008073', '800006523', '800002297', '800002872', '800006741', '800007380']\n",
    "\n",
    "sMCB_skus = ['800006506', '800006505', '800006527', '800006526', '800006525', '800007546', '800007583', '800007839',\n",
    "             '800006524', '800006627', '800007872']\n",
    "\n",
    "vial_skus = ['800004400', '800004401', '800006626', '800006740', '800007996']\n",
    "\n",
    "aspart_dlp_skus = ['800008016', '800002958', '800002948', '800006528', '800002989', '800003528', '800006592', '800006691']\n",
    "\n",
    "aspart_vial_skus = ['800008017', '800006529']\n",
    "\n",
    "rhi_skus = ['800001300', '800001298', '800001299']\n",
    "\n",
    "for sku in mCB_skus:\n",
    "    product_mapping[sku] = {'assembly': '700003964', 'filling': '700001012', 'root': '700001470', 'family': 'Glargine_mCB_DLP'}\n",
    "\n",
    "for sku in sMCB_skus:\n",
    "    product_mapping[sku] = {'assembly': '700004129', 'filling': '700004130', 'root': '700004130', 'family': 'Glargine_sMCB_DLP_EU'}\n",
    "\n",
    "for sku in vial_skus:\n",
    "    product_mapping[sku] = {'assembly': '700001123', 'filling': '700001123', 'root': '700001123', 'family': 'Glargine_Vial'}\n",
    "\n",
    "for sku in aspart_dlp_skus:\n",
    "    product_mapping[sku] = {'assembly': '700002770', 'filling': '700001301', 'root': '700001301', 'family': 'Aspart_DLP'}\n",
    "\n",
    "for sku in aspart_vial_skus:\n",
    "    product_mapping[sku] = {'assembly': '700001318', 'filling': '700001318', 'root': '700001318', 'family': 'Aspart_Vial'}\n",
    "\n",
    "for sku in rhi_skus:\n",
    "    product_mapping[sku] = {'assembly': '700000536', 'filling': '700000536', 'root': '700000536', 'family': 'RHI'}\n",
    "\n",
    "print(f\"Hierarchy defined with {len(product_mapping)} SKU mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating per-SKU pegging sheets...\")\n",
    "\n",
    "output_cols = ['BOM_Type', 'BOM_Level', 'Product_ID', 'Product_Description', 'SKU', 'Country', 'Pack_Size',\n",
    "               'Material', 'Material_Description', 'QTY', 'Section', 'Product_Family', 'Common_Unique',\n",
    "               'Total_Lead_Time', 'BUoM', 'Model', 'Resource_ID', 'Resource_Description', 'Batch_Size']\n",
    "\n",
    "sku_pegging_sheets = {}\n",
    "\n",
    "for market_sku, mapping_info in sorted(product_mapping.items()):\n",
    "    assembly_id = mapping_info.get('assembly')\n",
    "    filling_id = mapping_info.get('filling')\n",
    "    family_name = mapping_info.get('family', 'Unknown')\n",
    "    \n",
    "    pegging_data = []\n",
    "    \n",
    "    if market_sku in product_materials:\n",
    "        for _, mat_row in product_materials[market_sku].iterrows():\n",
    "            sku_info = sku_data.get(market_sku, {})\n",
    "            product_info = product_headers.get(market_sku, {})\n",
    "            resource_info = resource_data.get(market_sku, {})\n",
    "            \n",
    "            pegging_data.append({\n",
    "                'BOM_Type': 'Packing',\n",
    "                'BOM_Level': 'L1_Market_SKU',\n",
    "                'Product_ID': market_sku,\n",
    "                'Product_Description': product_info.get('Product_Description', 'N/A'),\n",
    "                'SKU': sku_info.get('SKU', 'N/A'),\n",
    "                'Country': sku_info.get('Country', 'N/A'),\n",
    "                'Pack_Size': sku_info.get('Pack_Size', 'N/A'),\n",
    "                'Material': normalize_text(mat_row['Material_Normalized']),\n",
    "                'Material_Description': normalize_text(mat_row['Material_Description']),\n",
    "                'QTY': mat_row['QTY'],\n",
    "                'Section': normalize_text(mat_row['Section']),\n",
    "                'Product_Family': family_name,\n",
    "                'Common_Unique': normalize_text(mat_row['Common_Unique']),\n",
    "                'Total_Lead_Time': mat_row['Total_Lead_Time'],\n",
    "                'BUoM': normalize_text(mat_row['BUoM']),\n",
    "                'Model': extract_model_components(mat_row['Model']),\n",
    "                'Resource_ID': resource_info.get('Resource_ID', 'N/A'),\n",
    "                'Resource_Description': resource_info.get('Resource_Description', 'N/A'),\n",
    "                'Batch_Size': product_info.get('Batch_Size', 'N/A')\n",
    "            })\n",
    "    \n",
    "    pegging_data.append({col: '0' if col == 'Product_ID' else None for col in output_cols})\n",
    "    \n",
    "    if assembly_id != market_sku and assembly_id in product_materials:\n",
    "        for _, mat_row in product_materials[assembly_id].iterrows():\n",
    "            product_info = product_headers.get(assembly_id, {})\n",
    "            resource_info = resource_data.get(assembly_id, {})\n",
    "            \n",
    "            pegging_data.append({\n",
    "                'BOM_Type': 'Assembly',\n",
    "                'BOM_Level': 'L2_Assembly',\n",
    "                'Product_ID': assembly_id,\n",
    "                'Product_Description': product_info.get('Product_Description', 'N/A'),\n",
    "                'SKU': 'N/A',\n",
    "                'Country': 'N/A',\n",
    "                'Pack_Size': 'N/A',\n",
    "                'Material': normalize_text(mat_row['Material_Normalized']),\n",
    "                'Material_Description': normalize_text(mat_row['Material_Description']),\n",
    "                'QTY': mat_row['QTY'],\n",
    "                'Section': normalize_text(mat_row['Section']),\n",
    "                'Product_Family': family_name,\n",
    "                'Common_Unique': normalize_text(mat_row['Common_Unique']),\n",
    "                'Total_Lead_Time': mat_row['Total_Lead_Time'],\n",
    "                'BUoM': normalize_text(mat_row['BUoM']),\n",
    "                'Model': extract_model_components(mat_row['Model']),\n",
    "                'Resource_ID': resource_info.get('Resource_ID', 'N/A'),\n",
    "                'Resource_Description': resource_info.get('Resource_Description', 'N/A'),\n",
    "                'Batch_Size': product_info.get('Batch_Size', 'N/A')\n",
    "            })\n",
    "    \n",
    "    pegging_data.append({col: '0' if col == 'Product_ID' else None for col in output_cols})\n",
    "    \n",
    "    if filling_id != assembly_id and filling_id in product_materials:\n",
    "        for _, mat_row in product_materials[filling_id].iterrows():\n",
    "            product_info = product_headers.get(filling_id, {})\n",
    "            resource_info = resource_data.get(filling_id, {})\n",
    "            \n",
    "            pegging_data.append({\n",
    "                'BOM_Type': 'Filling',\n",
    "                'BOM_Level': 'L3_Filling',\n",
    "                'Product_ID': filling_id,\n",
    "                'Product_Description': product_info.get('Product_Description', 'N/A'),\n",
    "                'SKU': 'N/A',\n",
    "                'Country': 'N/A',\n",
    "                'Pack_Size': 'N/A',\n",
    "                'Material': normalize_text(mat_row['Material_Normalized']),\n",
    "                'Material_Description': normalize_text(mat_row['Material_Description']),\n",
    "                'QTY': mat_row['QTY'],\n",
    "                'Section': normalize_text(mat_row['Section']),\n",
    "                'Product_Family': family_name,\n",
    "                'Common_Unique': normalize_text(mat_row['Common_Unique']),\n",
    "                'Total_Lead_Time': mat_row['Total_Lead_Time'],\n",
    "                'BUoM': normalize_text(mat_row['BUoM']),\n",
    "                'Model': extract_model_components(mat_row['Model']),\n",
    "                'Resource_ID': resource_info.get('Resource_ID', 'N/A'),\n",
    "                'Resource_Description': resource_info.get('Resource_Description', 'N/A'),\n",
    "                'Batch_Size': product_info.get('Batch_Size', 'N/A')\n",
    "            })\n",
    "    \n",
    "    df_pegging = pd.DataFrame(pegging_data)[output_cols]\n",
    "    sku_pegging_sheets[market_sku] = df_pegging\n",
    "\n",
    "print(f\"Created pegging sheets for {len(sku_pegging_sheets)} market SKUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting to Excel...\")\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for idx, (sku, pegging_df) in enumerate(sorted(sku_pegging_sheets.items()), 1):\n",
    "        sheet_name = f\"Pegging_{sku}\"[:31]\n",
    "        pegging_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        if idx % 10 == 0 or idx == len(sku_pegging_sheets):\n",
    "            print(f\"  Exported {idx}/{len(sku_pegging_sheets)} SKU pegging sheets\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nExported: {output_file}\")\n",
    "print(f\"Total sheets: {len(sku_pegging_sheets)}\")\n",
    "print(f\"Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PER-SKU PEGGING SUMMARY\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "print(f\"\\nFILE: Material_pegging_SKU_2_BOM.xlsx\")\n",
    "print(f\"\\nTOTAL SKU PEGGING SHEETS: {len(sku_pegging_sheets)}\")\n",
    "\n",
    "print(f\"\\nSHEET STRUCTURE (Per Market SKU):\")\n",
    "print(f\"  Level 1 (Packing): Market SKU materials\")\n",
    "print(f\"  Separator: 0 (Product_ID column only)\")\n",
    "print(f\"  Level 2 (Assembly): Assembly materials\")\n",
    "print(f\"  Separator: 0 (Product_ID column only)\")\n",
    "print(f\"  Level 3 (Filling): Filling materials\")\n",
    "\n",
    "print(f\"\\nCOLUMNS (19 total):\")\n",
    "for idx, col in enumerate(output_cols, 1):\n",
    "    print(f\"  {idx:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nEach market SKU has complete hierarchical BOM with Packing -> Assembly -> Filling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Material Availibility and SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- File + Sheet configuration ----\n",
    "dp_file = \"/home/supriyo/Downloads/Biocon_nw/20251006-DP Material Shortage - Working file.xlsx\"\n",
    "sheet_name = \"DP Shortage\"\n",
    "\n",
    "# ---- Read DP Shortage sheet ----\n",
    "# Header is on row 22 (Excel) → index 21 in pandas\n",
    "df = pd.read_excel(dp_file, sheet_name=sheet_name, header=21)\n",
    "\n",
    "# Restrict to valid data rows (23–541 in Excel → index 22–540)\n",
    "df = df.iloc[22:541].copy()\n",
    "\n",
    "# ---- Column Mapping ----\n",
    "output = pd.DataFrame()\n",
    "output[\"material_id\"] = df[\"Material\"]\n",
    "output[\"material_description\"] = df[\"Material Description\"]\n",
    "\n",
    "# Optional SS_DOS column placeholder\n",
    "output[\"SS_DOS\"] = np.nan      # Can be filled later if required\n",
    "\n",
    "# SS_QTY → from Total Safety Stock (col ER)\n",
    "output[\"SS_QTY\"] = df.get(\"Total Safety Stock\", 0).fillna(0)\n",
    "\n",
    "# UBOM\n",
    "output[\"UBOM\"] = df.get(\"BUoM\")\n",
    "\n",
    "# ---- Compute Available_QTY ----\n",
    "output[\"Available_QTY\"] = (\n",
    "    df.get(\"Unrestricted Stock\", 0).fillna(0)\n",
    "    - df.get(\"Total Safety Stock\", 0).fillna(0)\n",
    "    - df.get(\"QI Stock\", 0).fillna(0)\n",
    "    - df.get(\"Rejected RJ1\", 0).fillna(0)\n",
    "    - df.get(\"Expired Material 2001\", 0).fillna(0)\n",
    "    - df.get(\"Expired Material QI\", 0).fillna(0)\n",
    "    - df.get(\"Transfer to Process Order\", 0).fillna(0)\n",
    ")\n",
    "\n",
    "# Prevent negative available qty\n",
    "output[\"Available_QTY\"] = output[\"Available_QTY\"].clip(lower=0)\n",
    "\n",
    "# ---- Save Output ----\n",
    "save_path = \"/home/supriyo/Downloads/Biocon_nw/Product_master_inv.xlsx\"\n",
    "output.to_excel(save_path, index=False)\n",
    "\n",
    "print(f\"✅ Product inventory master saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Product_Demand**  **Planned_Production_FG** **BOM_Demand_By_Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------\n",
    "# File paths / sheet names\n",
    "# ------------------------------\n",
    "dp_file = \"/home/supriyo/Downloads/Biocon_nw/Aug'25_L2_DP_Plan_Circulation_V2.xlsx\"\n",
    "dp_sheet = \"L2Ph1_Detail\"\n",
    "\n",
    "rccp_file = \"/home/supriyo/Downloads/Biocon_nw/ParkourSC_SNP.xlsx\"\n",
    "rccp_sheet = \"DP RCCP\"\n",
    "\n",
    "shortage_file = \"/home/supriyo/Downloads/Biocon_nw/20251006-DP Material Shortage - Working file.xlsx\"\n",
    "shortage_sheet = \"DP Shortage\"\n",
    "\n",
    "out_file = \"/home/supriyo/Downloads/Biocon_nw/Product_master_inv.xlsx\"\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "MONTH_MAP = {\n",
    "    \"JAN\": \"01\", \"FEB\": \"02\", \"MAR\": \"03\", \"APR\": \"04\", \"MAY\": \"05\", \"JUN\": \"06\",\n",
    "    \"JUL\": \"07\", \"AUG\": \"08\", \"SEP\": \"09\", \"SEPT\": \"09\", \"OCT\": \"10\", \"NOV\": \"11\", \"DEC\": \"12\"\n",
    "}\n",
    "\n",
    "def norm_month_col(name: str):\n",
    "    \"\"\"\n",
    "    Normalize headers like 'OCT 2025', 'Nov-2026', 'Dec 25', 'Jun-27' to 'YYYY-MM'.\n",
    "    Returns original if not a month.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    s = name.strip().upper().replace(\"’\", \"'\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    # Patterns:\n",
    "    # 1) 'OCT 2025'\n",
    "    m = re.match(r\"^(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|SEPT|OCT|NOV|DEC)[\\s\\-]+(\\d{2,4})$\", s)\n",
    "    if m:\n",
    "        mon, yr = m.group(1), m.group(2)\n",
    "        yr = (\"20\" + yr) if len(yr) == 2 else yr\n",
    "        return f\"{yr}-{MONTH_MAP[mon]}\"\n",
    "    # 2) '2025 Oct' (unlikely here but safe)\n",
    "    m = re.match(r\"^(\\d{4})[\\s\\-]+(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|SEPT|OCT|NOV|DEC)$\", s)\n",
    "    if m:\n",
    "        yr, mon = m.group(1), m.group(2)\n",
    "        return f\"{yr}-{MONTH_MAP[mon]}\"\n",
    "    return name\n",
    "\n",
    "def to_numeric_safe(series):\n",
    "    return pd.to_numeric(series, errors=\"coerce\").fillna(0)\n",
    "\n",
    "def excel_cols_to_index(df, first_letter: str, last_letter: str):\n",
    "    \"\"\"\n",
    "    Map Excel column letters (e.g., 'S' to 'AM') to integer index slices for current df.columns.\n",
    "    We rely on physical column positions here.\n",
    "    \"\"\"\n",
    "    def letter_to_num(col):\n",
    "        col = col.upper()\n",
    "        num = 0\n",
    "        for ch in col:\n",
    "            num = num * 26 + (ord(ch) - ord('A') + 1)\n",
    "        return num - 1  # zero-based\n",
    "    start = letter_to_num(first_letter)\n",
    "    end = letter_to_num(last_letter)\n",
    "    # Bound by df shape\n",
    "    start = max(0, min(start, df.shape[1]-1))\n",
    "    end = max(0, min(end, df.shape[1]-1))\n",
    "    return slice(start, end+1)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Read L2Ph1_Detail (rows 3–78)\n",
    "# ------------------------------\n",
    "# We don't know the exact header row index in the file, but you said row 1-2 are non-data and row 3 starts data.\n",
    "# We'll read the whole sheet, then slice rows 2:78 (0-based -> rows 3..78 inclusive).\n",
    "dp_raw = pd.read_excel(dp_file, sheet_name=dp_sheet, engine=\"openpyxl\", header=0)\n",
    "dp = dp_raw.iloc[2:78].copy()  # rows 3–78 (exclusive of 78 if less rows exist)\n",
    "\n",
    "# Normalize column names (especially months)\n",
    "dp.columns = [norm_month_col(c) for c in dp.columns]\n",
    "\n",
    "# Identify key columns by position per your note:\n",
    "# E = Market SKU (as per your instruction), K = Batch Size\n",
    "# We'll also try to pick Product ID from a likely column named 'Product ID' if present.\n",
    "col_letters = {name: idx for idx, name in enumerate(dp.columns)}\n",
    "\n",
    "def col_by_letter(letter):\n",
    "    # approximate: take position in original raw header order\n",
    "    # Use dp_raw.columns (before normalization) to resolve positions\n",
    "    headers = list(dp_raw.columns)\n",
    "    def letter_to_num(col):\n",
    "        col = col.upper()\n",
    "        num = 0\n",
    "        for ch in col:\n",
    "            num = num * 26 + (ord(ch) - ord('A') + 1)\n",
    "        return num - 1\n",
    "    pos = letter_to_num(letter)\n",
    "    return headers[pos] if pos < len(headers) else None\n",
    "\n",
    "market_sku_col = col_by_letter(\"E\")\n",
    "batch_size_col = col_by_letter(\"K\")\n",
    "\n",
    "# Build month column ranges\n",
    "ea_cols_letters = (\"S\", \"AM\")  # EA S:AM\n",
    "batch_cols_letters = (\"AN\", \"BI\")  # Batches AN:BI\n",
    "\n",
    "ea_first = col_by_letter(ea_cols_letters[0])\n",
    "ea_last = col_by_letter(ea_cols_letters[1])\n",
    "batch_first = col_by_letter(batch_cols_letters[0])\n",
    "batch_last = col_by_letter(batch_cols_letters[1])\n",
    "\n",
    "# Convert those to normalized names after we normalized headers\n",
    "ea_start_idx = list(dp_raw.columns).index(ea_first) if ea_first in dp_raw.columns else None\n",
    "ea_end_idx = list(dp_raw.columns).index(ea_last) if ea_last in dp_raw.columns else None\n",
    "batch_start_idx = list(dp_raw.columns).index(batch_first) if batch_first in dp_raw.columns else None\n",
    "batch_end_idx = list(dp_raw.columns).index(batch_last) if batch_last in dp_raw.columns else None\n",
    "\n",
    "if None in (ea_start_idx, ea_end_idx, batch_start_idx, batch_end_idx):\n",
    "    raise RuntimeError(\"Could not resolve EA/Batch column spans; please verify the sheet layout.\")\n",
    "\n",
    "# Re-find by normalized column names\n",
    "norm_cols = list(dp.columns)\n",
    "ea_norm_cols = norm_cols[ea_start_idx:ea_end_idx+1]\n",
    "batch_norm_cols = norm_cols[batch_start_idx:batch_end_idx+1]\n",
    "\n",
    "# Prepare base ID columns\n",
    "id_cols = []\n",
    "if \"Product ID\" in dp.columns:\n",
    "    id_cols.append(\"Product ID\")\n",
    "if market_sku_col and market_sku_col in dp_raw.columns:\n",
    "    mk_norm = norm_month_col(market_sku_col)\n",
    "    # If normalization didn't change (most likely), keep original header\n",
    "    id_cols.append(market_sku_col if market_sku_col in dp.columns else mk_norm)\n",
    "\n",
    "# Include product description if available\n",
    "for cand in [\"Product Desc\", \"Product Description\"]:\n",
    "    if cand in dp.columns:\n",
    "        id_cols.append(cand)\n",
    "\n",
    "# Batch size\n",
    "if batch_size_col in dp_raw.columns:\n",
    "    bs_norm = norm_month_col(batch_size_col)\n",
    "    if bs_norm in dp.columns:\n",
    "        id_cols.append(bs_norm)\n",
    "        batch_size_norm_name = bs_norm\n",
    "    else:\n",
    "        id_cols.append(batch_size_col)\n",
    "        batch_size_norm_name = batch_size_col\n",
    "else:\n",
    "    batch_size_norm_name = None\n",
    "\n",
    "# Deduplicate while preserving order\n",
    "seen = set()\n",
    "id_cols = [c for c in id_cols if not (c in seen or seen.add(c))]\n",
    "\n",
    "# Melt EA\n",
    "ea_df = dp[id_cols + ea_norm_cols].copy()\n",
    "ea_long = ea_df.melt(id_vars=id_cols, value_vars=ea_norm_cols, var_name=\"Month\", value_name=\"Demand_EA\")\n",
    "ea_long[\"Demand_EA\"] = to_numeric_safe(ea_long[\"Demand_EA\"])\n",
    "\n",
    "# Melt Batches\n",
    "batch_df = dp[id_cols + batch_norm_cols].copy()\n",
    "batch_long = batch_df.melt(id_vars=id_cols, value_vars=batch_norm_cols, var_name=\"Month\", value_name=\"Demand_Batches\")\n",
    "batch_long[\"Demand_Batches\"] = to_numeric_safe(batch_long[\"Demand_Batches\"])\n",
    "\n",
    "# Merge EA + Batches\n",
    "prod_demand = pd.merge(ea_long, batch_long, on=id_cols + [\"Month\"], how=\"outer\")\n",
    "prod_demand[\"Month\"] = prod_demand[\"Month\"].apply(norm_month_col)\n",
    "\n",
    "# Rename key ID columns to canonical names\n",
    "rename_map = {}\n",
    "if \"Product ID\" in prod_demand.columns:\n",
    "    rename_map[\"Product ID\"] = \"Product_ID\"\n",
    "if market_sku_col in prod_demand.columns:\n",
    "    rename_map[market_sku_col] = \"Market_SKU\"\n",
    "if \"Product Desc\" in prod_demand.columns:\n",
    "    rename_map[\"Product Desc\"] = \"Product_Desc\"\n",
    "if \"Product Description\" in prod_demand.columns:\n",
    "    rename_map[\"Product Description\"] = \"Product_Desc\"\n",
    "if batch_size_norm_name and batch_size_norm_name in prod_demand.columns:\n",
    "    rename_map[batch_size_norm_name] = \"Batch_Size\"\n",
    "\n",
    "prod_demand.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 2) RCCP: Planned Production & Outbound Production Demand\n",
    "# ------------------------------\n",
    "rccp = pd.read_excel(rccp_file, sheet_name=rccp_sheet, engine=\"openpyxl\", header=0)\n",
    "\n",
    "# Normalize month headers\n",
    "rccp.columns = [norm_month_col(c) for c in rccp.columns]\n",
    "\n",
    "# Column identities (by names in your description)\n",
    "# Product ID in col C (we'll rely on column name 'Product ID' if present; else 3rd col)\n",
    "if \"Product ID\" in rccp.columns:\n",
    "    rccp[\"Product_ID\"] = rccp[\"Product ID\"]\n",
    "else:\n",
    "    rccp[\"Product_ID\"] = rccp.iloc[:, 2]  # 3rd col\n",
    "\n",
    "# Key Figure in col H (or by name)\n",
    "key_col = \"Key Figure\" if \"Key Figure\" in rccp.columns else rccp.columns[7]\n",
    "rccp[\"Key_Figure\"] = rccp[key_col]\n",
    "\n",
    "# Month spread J..AD = March 2025 .. April 2027: Normalize and collect all YYYY-MM columns\n",
    "month_cols = [c for c in rccp.columns if isinstance(c, str) and re.match(r\"^\\d{4}\\-\\d{2}$\", c)]\n",
    "month_cols = sorted(month_cols)  # chronological\n",
    "\n",
    "# Filter Filled FG PIDs\n",
    "filled_fg_pids = {\"700001012\",\"700001123\",\"700000536\",\"700001318\",\"700001301\"}\n",
    "exclude_pids = {\"700004130\"}\n",
    "\n",
    "rccp_filled = rccp[\n",
    "    (rccp[\"Product_ID\"].astype(str).isin(filled_fg_pids)) &\n",
    "    (~rccp[\"Product_ID\"].astype(str).isin(exclude_pids))\n",
    "].copy()\n",
    "\n",
    "# Pick Planned Production\n",
    "pp = rccp_filled[rccp_filled[\"Key_Figure\"].str.strip().str.lower() == \"planned production\"].copy()\n",
    "pp_long = pp.melt(id_vars=[\"Product_ID\",\"Key_Figure\"], value_vars=month_cols, var_name=\"Month\", value_name=\"Planned_Production\")\n",
    "pp_long[\"Planned_Production\"] = to_numeric_safe(pp_long[\"Planned_Production\"])\n",
    "\n",
    "# Pick Outbound Production Demand (used to explode to components)\n",
    "opd = rccp_filled[rccp_filled[\"Key_Figure\"].str.strip().str.lower() == \"outbound production demand\"].copy()\n",
    "opd_long = opd.melt(id_vars=[\"Product_ID\",\"Key_Figure\"], value_vars=month_cols, var_name=\"Month\", value_name=\"Outbound_Production_Demand\")\n",
    "opd_long[\"Outbound_Production_Demand\"] = to_numeric_safe(opd_long[\"Outbound_Production_Demand\"])\n",
    "\n",
    "# Merge PP & OPD\n",
    "pp_opd = pd.merge(pp_long.drop(columns=[\"Key_Figure\"]),\n",
    "                  opd_long.drop(columns=[\"Key_Figure\"]),\n",
    "                  on=[\"Product_ID\",\"Month\"], how=\"outer\").fillna(0)\n",
    "\n",
    "# ------------------------------\n",
    "# 3) BOM component mapping from DP Shortage (rows 23–542)\n",
    "#     - \"Product_ID\" match in that sheet\n",
    "#     - Component Material in Col F\n",
    "# ------------------------------\n",
    "shortage_raw = pd.read_excel(shortage_file, sheet_name=shortage_sheet, engine=\"openpyxl\", header=0)\n",
    "shortage = shortage_raw.iloc[22:542].copy()  # rows 23–542\n",
    "\n",
    "# Try to identify columns:\n",
    "# Product_ID column: often named 'Model' or 'Product ID' in previous contexts. Prefer exact match if present.\n",
    "probable_pid_cols = [c for c in shortage.columns if str(c).strip().lower() in {\"product id\", \"product_id\", \"model\", \"parent_product_id\"}]\n",
    "if probable_pid_cols:\n",
    "    shortage[\"Product_ID\"] = shortage[probable_pid_cols[0]]\n",
    "else:\n",
    "    # Fallback: take column 'D' (4th) if exists\n",
    "    if shortage.shape[1] >= 4:\n",
    "        shortage[\"Product_ID\"] = shortage.iloc[:, 3]\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not find Product_ID column in DP Shortage.\")\n",
    "\n",
    "# Component material in Col F (6th col) per your note\n",
    "comp_col = shortage.columns[5] if shortage.shape[1] >= 6 else None\n",
    "if comp_col is None:\n",
    "    raise RuntimeError(\"Could not resolve Component Material column (expected column F).\")\n",
    "\n",
    "shortage[\"Component_Material_ID\"] = shortage[comp_col]\n",
    "\n",
    "# Keep non-blank, non-zero Product_ID & Component\n",
    "bom_map = shortage[\n",
    "    shortage[\"Product_ID\"].notna() & (shortage[\"Product_ID\"] != 0) &\n",
    "    shortage[\"Component_Material_ID\"].notna() & (shortage[\"Component_Material_ID\"] != 0)\n",
    "].copy()\n",
    "\n",
    "# Filter to the filled FG PIDs only (as per instruction)\n",
    "bom_map = bom_map[bom_map[\"Product_ID\"].astype(str).isin(filled_fg_pids)]\n",
    "\n",
    "# Deduplicate product-component pairs\n",
    "bom_pairs = bom_map[[\"Product_ID\",\"Component_Material_ID\"]].drop_duplicates()\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Create Component-level monthly demand by joining OPD with BOM pairs\n",
    "#     Assumption: BOM factor = 1 (until you provide exact BOM factors)\n",
    "# ------------------------------\n",
    "comp_demand = bom_pairs.merge(pp_opd, on=\"Product_ID\", how=\"left\")\n",
    "\n",
    "# If you have factors later, multiply here:\n",
    "# comp_demand[\"Component_Monthly_Demand\"] = comp_demand[\"Outbound_Production_Demand\"] * comp_demand[\"BOM_Factor\"]\n",
    "comp_demand[\"Component_Monthly_Demand\"] = comp_demand[\"Outbound_Production_Demand\"]\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Product mapping dictionary (family, assembly, filling, root)\n",
    "# ------------------------------\n",
    "product_mapping = {}\n",
    "\n",
    "mCB_skus = ['800004403', '800004402', '800008019', '800008020', '800008034', '800007997', '800007345', '800007516',\n",
    "            '800002513', '800007608', '800007630', '800002984', '800004986', '800007310', '800007311', '800006648',\n",
    "            '800007634', '800008073', '800006523', '800002297', '800002872', '800006741', '800007380']\n",
    "\n",
    "sMCB_skus = ['800006506', '800006505', '800006527', '800006526', '800006525', '800007546', '800007583', '800007839',\n",
    "             '800006524', '800006627', '800007872']\n",
    "\n",
    "vial_skus = ['800004400', '800004401', '800006626', '800006740', '800007996']\n",
    "\n",
    "aspart_dlp_skus = ['800008016', '800002958', '800002948', '800006528', '800002989', '800003528', '800006592', '800006691']\n",
    "\n",
    "aspart_vial_skus = ['800008017', '800006529']\n",
    "\n",
    "rhi_skus = ['800001300', '800001298', '800001299']\n",
    "\n",
    "for sku in mCB_skus:\n",
    "    product_mapping[sku] = {'assembly': '700003964', 'filling': '700001012', 'root': '700001470', 'family': 'Glargine_mCB_DLP'}\n",
    "\n",
    "for sku in sMCB_skus:\n",
    "    product_mapping[sku] = {'assembly': '700004129', 'filling': '700004130', 'root': '700004130', 'family': 'Glargine_sMCB_DLP_EU'}\n",
    "\n",
    "for sku in vial_skus:\n",
    "    product_mapping[sku] = {'assembly': '700001123', 'filling': '700001123', 'root': '700001123', 'family': 'Glargine_Vial'}\n",
    "\n",
    "for sku in aspart_dlp_skus:\n",
    "    product_mapping[sku] = {'assembly': '700002770', 'filling': '700001301', 'root': '700001301', 'family': 'Aspart_DLP'}\n",
    "\n",
    "for sku in aspart_vial_skus:\n",
    "    product_mapping[sku] = {'assembly': '700001318', 'filling': '700001318', 'root': '700001318', 'family': 'Aspart_Vial'}\n",
    "\n",
    "for sku in rhi_skus:\n",
    "    product_mapping[sku] = {'assembly': '700000536', 'filling': '700000536', 'root': '700000536', 'family': 'RHI'}\n",
    "\n",
    "# Attach mapping to Product_Demand where Market_SKU is available\n",
    "if \"Market_SKU\" in prod_demand.columns:\n",
    "    map_df = pd.DataFrame(product_mapping).T.reset_index().rename(columns={\"index\": \"Market_SKU\"})\n",
    "    prod_demand = prod_demand.merge(map_df, on=\"Market_SKU\", how=\"left\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Write outputs\n",
    "# ------------------------------\n",
    "with pd.ExcelWriter(out_file, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "    # Product_Demand (normalized long form with EA + Batches)\n",
    "    cols_order = [c for c in [\"Product_ID\",\"Market_SKU\",\"Product_Desc\",\"Batch_Size\",\"Month\",\"Demand_EA\",\"Demand_Batches\",\n",
    "                              \"family\",\"assembly\",\"filling\",\"root\"] if c in prod_demand.columns]\n",
    "    prod_demand[cols_order].to_excel(writer, sheet_name=\"Product_Demand\", index=False)\n",
    "\n",
    "    # RCCP Planned Production / OPD (filtered to filled FGs)\n",
    "    pp_opd.sort_values([\"Product_ID\",\"Month\"]).to_excel(writer, sheet_name=\"Planned_Production_FG\", index=False)\n",
    "\n",
    "    # Component-level monthly demand (from OPD mapped via BOM)\n",
    "    comp_cols = [\"Product_ID\",\"Component_Material_ID\",\"Month\",\"Outbound_Production_Demand\",\"Component_Monthly_Demand\"]\n",
    "    comp_demand.sort_values([\"Product_ID\",\"Component_Material_ID\",\"Month\"]).to_excel(writer, sheet_name=\"BOM_Demand_By_Component\", index=False)\n",
    "\n",
    "print(f\"Success: wrote Product_Demand, Planned_Production_FG, BOM_Demand_By_Component to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "MATERIAL PEGGING MAP - PER-SKU PEGGING WITH COMPLETE HIERARCHY\n",
      "========================================================================================================================\n",
      "Found 52 pegging sheets in /home/supriyo/Downloads/Biocon_nw/Material_pegging_SKU_2_BOM.xlsx\n",
      "  - Pegging_800001298\n",
      "  - Pegging_800001299\n",
      "  - Pegging_800001300\n",
      "  - Pegging_800002297\n",
      "  - Pegging_800002513\n",
      "  - Pegging_800002872\n",
      "  - Pegging_800002948\n",
      "  - Pegging_800002958\n",
      "  - Pegging_800002984\n",
      "  - Pegging_800002989\n",
      "  - Pegging_800003528\n",
      "  - Pegging_800004400\n",
      "  - Pegging_800004401\n",
      "  - Pegging_800004402\n",
      "  - Pegging_800004403\n",
      "  - Pegging_800004986\n",
      "  - Pegging_800006505\n",
      "  - Pegging_800006506\n",
      "  - Pegging_800006523\n",
      "  - Pegging_800006524\n",
      "  - Pegging_800006525\n",
      "  - Pegging_800006526\n",
      "  - Pegging_800006527\n",
      "  - Pegging_800006528\n",
      "  - Pegging_800006529\n",
      "  - Pegging_800006592\n",
      "  - Pegging_800006626\n",
      "  - Pegging_800006627\n",
      "  - Pegging_800006648\n",
      "  - Pegging_800006691\n",
      "  - Pegging_800006740\n",
      "  - Pegging_800006741\n",
      "  - Pegging_800007310\n",
      "  - Pegging_800007311\n",
      "  - Pegging_800007345\n",
      "  - Pegging_800007380\n",
      "  - Pegging_800007516\n",
      "  - Pegging_800007546\n",
      "  - Pegging_800007583\n",
      "  - Pegging_800007608\n",
      "  - Pegging_800007630\n",
      "  - Pegging_800007634\n",
      "  - Pegging_800007839\n",
      "  - Pegging_800007872\n",
      "  - Pegging_800007996\n",
      "  - Pegging_800007997\n",
      "  - Pegging_800008016\n",
      "  - Pegging_800008017\n",
      "  - Pegging_800008019\n",
      "  - Pegging_800008020\n",
      "  - Pegging_800008034\n",
      "  - Pegging_800008073\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"MATERIAL PEGGING MAP - PER-SKU PEGGING WITH COMPLETE HIERARCHY\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "dp_file = \"/home/supriyo/Downloads/Biocon_nw/20251006-DP Material Shortage - Working file.xlsx\"\n",
    "snp_file = \"/home/supriyo/Downloads/Biocon_nw/ParkourSC_SNP.xlsx\"\n",
    "pegging_file = \"/home/supriyo/Downloads/Biocon_nw/Material_pegging_SKU_2_BOM.xlsx\"\n",
    "output_file = \"/home/supriyo/Downloads/Biocon_nw/pegging_output.xlsx\"\n",
    "\n",
    "xls = pd.ExcelFile(pegging_file, engine='openpyxl')\n",
    "\n",
    "# filter sheet nmaes that start with Pegging_\n",
    "pegging_sheets = [sheet for sheet in xls.sheet_names if sheet.startswith(\"Pegging_\")]\n",
    "\n",
    "print(f\"Found {len(pegging_sheets)} pegging sheets in {pegging_file}\")\n",
    "for sheet in pegging_sheets:\n",
    "    print(f\"  - {sheet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
